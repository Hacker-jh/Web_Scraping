{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi all! This notebook is my compiled and tidied version of basic web scraping with python using Beautiful Soup.\n\nThe following is my experimental subject\n\nhttps://www.linkedin.com/learning/search?trk=homepage-basic_intent-module-learning&sortBy=RELEVANCE&entityType=COURSE","metadata":{}},{"cell_type":"code","source":"# importing the libraries\nimport requests # to allow http requests\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport os\nimport urllib.request as request","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T07:54:40.061345Z","iopub.execute_input":"2022-06-16T07:54:40.061719Z","iopub.status.idle":"2022-06-16T07:54:40.06727Z","shell.execute_reply.started":"2022-06-16T07:54:40.061689Z","shell.execute_reply":"2022-06-16T07:54:40.066236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# begin with the url of the list of courses\nurl = 'https://www.linkedin.com/learning/search?trk=homepage-basic_intent-module-learning&sortBy=RELEVANCE&entityType=COURSE'\n# req = requests.get(url).text\n# soup = BeautifulSoup(req)\n# courses = soup.find_all('li',{'class':'results-list__item'})\n\n\n\n#ssr配置代理\nproxies={'http': 'http://127.0.0.1:4780', 'https': 'http://127.0.0.1:4780'}\nurl='https://www.linkedin.com/learning/search?trk=homepage-basic_intent-module-learning&sortBy=RELEVANCE&entityType=COURSE'\nr = requests.get(url,proxies=proxies)\n\n\nsoup = BeautifulSoup(r)\ncourses = soup.find_all('li',{'class':'results-list__item'})","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.068727Z","iopub.execute_input":"2022-06-16T07:54:40.069107Z","iopub.status.idle":"2022-06-16T07:54:40.113113Z","shell.execute_reply.started":"2022-06-16T07:54:40.069075Z","shell.execute_reply":"2022-06-16T07:54:40.111052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty arrays for each course seen on that list\nurlList = []\ndurationList = []\nnameList = []\nbyList = []\nviewCountList = []\nreleaseDateList = []\n\n# loop through the courses to extract information\nfor course in courses:\n    url = course.find(href=True)\n    urlList.append(url['href'])\n    name = course.find('h3',{'class':'base-search-card__title'}).text.strip()\n    nameList.append(name)\n    by = course.find('h4',{'class':'base-search-card__subtitle'}).text.strip()\n    byList.append(by)\n    duration = course.find('div',{'class':'search-entity-media__duration'}).text.strip()\n    durationList.append(duration)\n    metadataItem = course.find_all('span')\n    if \"Released\" in metadataItem[0].text:\n        viewCountList.append(0)\n        releaseDateList.append(metadataItem[0].text)\n    else:\n        viewCountList.append(metadataItem[0].text)\n        releaseDateList.append(metadataItem[1].text)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.114399Z","iopub.status.idle":"2022-06-16T07:54:40.115053Z","shell.execute_reply.started":"2022-06-16T07:54:40.114844Z","shell.execute_reply":"2022-06-16T07:54:40.114868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# form our dataframe from the obtained list\ndf = pd.DataFrame(list(zip(nameList,urlList,durationList,byList,viewCountList,releaseDateList)),\n                 columns = ['course name','url','duration','by','viewer count','release date'])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.116194Z","iopub.status.idle":"2022-06-16T07:54:40.116745Z","shell.execute_reply.started":"2022-06-16T07:54:40.116559Z","shell.execute_reply":"2022-06-16T07:54:40.116579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# empty arrays for items we could find from each course's url\nlikesList = []\nskillLevelList = []\nratingList = []\nratingMaxList = []\n\n# loop through each url to obtain information found on each webpage. If information not found, append None\nfor i in df['url']:\n    req = requests.get(i).text\n    soup = BeautifulSoup(req)\n    temp = soup.find_all('span',{'class':'top-card__headline-row-item'})\n    likes = None\n    skill = None\n    for i in temp:\n        if \"Liked\" in i.text:\n            likes = i.text\n        if \"Skill\" in i.text:\n            skill = i.text\n    likesList.append(likes)\n    skillLevelList.append(skill)\n    rating = soup.find('span',{'class':'ratings-summary__rating-average'})\n    if rating is not None:\n        ratingList.append(rating.text)\n    else:\n        ratingList.append(None)\n    ratingMax = soup.find('span',{'class':'ratings-summary__rating-max'})\n    if rating is not None:\n        ratingMaxList.append(ratingMax.text)\n    else:\n        ratingMaxList.append(None)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.117855Z","iopub.status.idle":"2022-06-16T07:54:40.118365Z","shell.execute_reply.started":"2022-06-16T07:54:40.118183Z","shell.execute_reply":"2022-06-16T07:54:40.118203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# form dataframe from obtained list\ntempdf = pd.DataFrame(list(zip(likesList,skillLevelList,ratingList,ratingMaxList)),\n                     columns = ['likes','skill level','rating','rating max'])\n\n# combine our two dataframe\ndf = pd.concat([df,tempdf],axis = 1)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.119261Z","iopub.status.idle":"2022-06-16T07:54:40.120401Z","shell.execute_reply.started":"2022-06-16T07:54:40.120193Z","shell.execute_reply":"2022-06-16T07:54:40.120214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save dataframe to csv file.\nos.chdir(r'/kaggle/working')\n\ndf.to_csv(r'webScrapexp.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T07:54:40.121629Z","iopub.status.idle":"2022-06-16T07:54:40.122178Z","shell.execute_reply.started":"2022-06-16T07:54:40.122004Z","shell.execute_reply":"2022-06-16T07:54:40.122025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the end of the page. Thanks for viewing!\n\nLink to the two more detailed notebook including my thought processes:\n\nhttps://www.kaggle.com/code/eugenetanake/basic-web-scraping-with-python-pt-1\n\nhttps://www.kaggle.com/code/eugenetanake/basic-web-scraping-with-python-pt-2","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}